{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NeuralMachine Translation with Attention\n"
      ],
      "metadata": {
        "id": "5drPixaNCX4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aim to develop/build an English-to-German neural machine\n",
        "translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention. Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation (e.g. determining whether the word \"bank\" refers to the financial bank, or the land alongside a river). Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. Also the Attention mechanism added to it, will help solve the vanishing gradients and equip it to build a long sequences capturing all the dependencies."
      ],
      "metadata": {
        "id": "jyy-qo0pCzQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the Dependencies"
      ],
      "metadata": {
        "id": "EsQiiCrfEN5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install trax\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training"
      ],
      "metadata": {
        "id": "pnIvxn1BCshG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading up the downloaded dataset"
      ],
      "metadata": {
        "id": "bHJZIjoGG00x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "#Check the directory\n",
        "!ls \"/content/drive/MyDrive/Project_Files\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vL2Oo1GDLw5",
        "outputId": "d28df82e-ec9e-4ecb-8fd7-d394c0830695"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "model.pkl.gz  source_data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.mkdir('/source_data')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/Project_Files/source_data.zip', 'r')\n",
        "zip_ref.extractall('source_data/')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "VVY_FbUBSa7e"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset would download from the TFDS dataset, the key is specified to bring the tuple of both English & Germany words\n",
        "\n",
        "train_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                 data_dir='./source_data/data/',\n",
        "                                 keys=('en', 'de'),\n",
        "                                 eval_holdout_size=0.01, # 1% for eval\n",
        "                                 train=True\n",
        "                                )\n",
        "\n",
        "# Get generator function for the eval set\n",
        "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                data_dir='./source_data/data/',\n",
        "                                keys=('en', 'de'),\n",
        "                                eval_holdout_size=0.01, # 1% for eval\n",
        "                                train=False\n",
        "                               )"
      ],
      "metadata": {
        "id": "-hw0qsvdGzxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display some examples of the data which would be working on"
      ],
      "metadata": {
        "id": "7VHRlI6_GzGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
        "print()\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUrzq9gzOWdB",
        "outputId": "b3692a22-e0ca-45e1-d784-66e772518f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data (en, de) tuple: (b'In the pregnant rat the AUC for calculated free drug at this dose was approximately 18 times the human AUC at a 20 mg dose.\\n', b'Bei tr\\xc3\\xa4chtigen Ratten war die AUC f\\xc3\\xbcr die berechnete ungebundene Substanz bei dieser Dosis etwa 18-mal h\\xc3\\xb6her als die AUC beim Menschen bei einer 20 mg Dosis.\\n')\n",
            "\n",
            "eval data (en, de) tuple: (b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the Data"
      ],
      "metadata": {
        "id": "HM1TPiWpAmYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Token and Formatting\n",
        "\n",
        "We need to have the data in a readable format for the machine to understand, therefore each of the sentences is tokenize and add to the vocab dictionary which would later be detokenize for readable output"
      ],
      "metadata": {
        "id": "Eb37iA68Oz9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables that state the filename and directory of the vocabulary file\n",
        "VOCAB_FILE = 'ende_32k.subword'\n",
        "VOCAB_DIR = 'source_data/data/'\n",
        "\n",
        "# Tokenize the dataset.\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ],
      "metadata": {
        "id": "9OFhg2w-PSf6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Append an end-of-sentence token to each sentence**: We will assign a token (i.e. in this case 1) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation."
      ],
      "metadata": {
        "id": "OqCIqFgpP5Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append EOS at the end of each sentence.\n",
        "\n",
        "# Integer assigned as end-of-sentence (EOS)\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ],
      "metadata": {
        "id": "M6j1M5jHPfr4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter long sentences**: We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the trax.data.FilterByLength() method and you can see its syntax below."
      ],
      "metadata": {
        "id": "8t5lfWN2Q4kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter too long sentences to not run out of memory.\n",
        "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
        "# both must be not longer than 512 tokens for training / 512 for eval.\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
        "print(colored(f'Single tokenized example target:', 'red'), train_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inO0Wc9DQvtM",
        "outputId": "25ce2cd1-2c70-4734-b3c3-558ce0aac119"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single tokenized example input: [  219  3550 30650  4729   992     1]\n",
            "Single tokenized example target: [  219  3550 30650  4729   992     1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating the Tokenize & De-Tokenize Function for any given input\n",
        "Given any data set, there is a need to map words to their indices, and indices to their words. The inputs and outputs to which would be supplied to the  trax models are usually tensors of numbers where each number corresponds to a word.\n",
        "  *   tokenize(): converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\n",
        "  *   detokenize(): converts a token list to its corresponding sentence (i.e. string).\n"
      ],
      "metadata": {
        "id": "bVlgBxbDkpa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup helper functions for tokenizing and detokenizing sentences\n",
        "\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "\n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "\n",
        "    # Mark the end of the sentence with EOS\n",
        "    inputs = list(inputs) + [EOS]\n",
        "\n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "\n",
        "    return batch_inputs\n",
        "\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove the dimensions of size 1\n",
        "    integers = list(np.squeeze(integers))\n",
        "\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "\n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)]\n",
        "\n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "metadata": {
        "id": "KOSJ1M8TmBMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the tokenize & de-tokenize function which was created above"
      ],
      "metadata": {
        "id": "IlMH_qgemfZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detokenize an input-target pair of tokenized sentences\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print()\n",
        "\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
        "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
        "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSPEAcG-mqdT",
        "outputId": "105ffb42-85b9-4012-9553-792724d1d45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single detokenized example input: Decreased Appetite\n",
            "\n",
            "Single detokenized example target: Verminderter Appetit\n",
            "\n",
            "\n",
            "tokenize('hello'):  [[17332   140     1]]\n",
            "detokenize([17332, 140, 1]):  hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bucketing\n",
        "This is needed to categorize the list of input with approximate equal length, this is done to save training time and for the effectiveness of the model"
      ],
      "metadata": {
        "id": "20L-5UsMnQic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "metadata": {
        "id": "2uofRgyWnK57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the data before feeding it into the model"
      ],
      "metadata": {
        "id": "5f5rgrNroj6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)\n",
        "\n",
        "#pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUstzzqCop7G",
        "outputId": "77210b5a-0a93-4be6-c8b7-83a136a9b1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (32, 64)\n",
            "target_batch shape:  (32, 64)\n",
            "THIS IS THE ENGLISH SENTENCE: \n",
            " Animal studies do not indicate direct or indirect harmful effects with respect to pregnancy, embryonal/ foetal development, parturition or postnatal development (see section 5.3).\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> \n",
            "\n",
            "THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            "  [26316  2127  4398   127    48  6470  1590    66 17257  7778  1743    30\n",
            "   997     9  3678 29055  1611     2 24146   278  6722 29265  5732   396\n",
            "     2  2825 22477   596    66  3575 28236   215   396    50   372  2745\n",
            "   194     3   199 33022 30650  4729   992     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n",
            "THIS IS THE GERMAN TRANSLATION: \n",
            " Tierexperimentelle Studien lassen nicht auf direkte oder indirekte schädliche Auswirkungen auf Schwangerschaft, embryonale/fetale Entwicklung, Geburt oder postnatale Entwicklung schließen (siehe Abschnitt 5.3).\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> \n",
            "\n",
            "THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
            " [ 5623 24019  6166 15779   530    44    37  8341    97  8847 10984   191\n",
            " 27529  1191    37 21145  3393  2121     2 24146  2985   123 17458  2985\n",
            "   398     2 16066     5    97  3575 28236   347   398  3800    50  4588\n",
            " 10729   194     3   199 33022 30650  4729   992     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Machine Translation with Attention"
      ],
      "metadata": {
        "id": "eTideyeEpKoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The input encoder"
      ],
      "metadata": {
        "id": "kTWakW1YpgPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\n",
        "    activations that will be the keys and values for attention.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    Returns:\n",
        "        tl.Serial: The input encoder\n",
        "    \"\"\"\n",
        "\n",
        "    # create a serial network\n",
        "    input_encoder = tl.Serial(\n",
        "\n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(input_vocab_size, d_model),\n",
        "\n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
        "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
        "\n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ],
      "metadata": {
        "id": "vm9bFV9YprVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The pre attention decoder"
      ],
      "metadata": {
        "id": "oFFQVikEp8Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
        "    activations that are used as queries in attention.\n",
        "\n",
        "    Args:\n",
        "        mode: str: 'train' or 'eval'\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    Returns:\n",
        "        tl.Serial: The pre-attention decoder\n",
        "    \"\"\"\n",
        "\n",
        "    # create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "\n",
        "        # shift right to insert start-of-sentence token and implement\n",
        "        # teacher forcing during training\n",
        "        tl.ShiftRight(mode = mode),\n",
        "\n",
        "        # run an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(target_vocab_size, d_model),\n",
        "\n",
        "        # feed to an LSTM layer\n",
        "        tl.LSTM(d_model)\n",
        "\n",
        "    )\n",
        "\n",
        "    return pre_attention_decoder"
      ],
      "metadata": {
        "id": "BMkTuD7cqEgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The attention input"
      ],
      "metadata": {
        "id": "YibDK1ZSqpbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
        "\n",
        "    Args:\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
        "        inputs fastnp.array(batch_size, padded_input_length): input tokens\n",
        "\n",
        "    Returns:\n",
        "        queries, keys, values and mask for attention.\n",
        "    \"\"\"\n",
        "\n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "\n",
        "\n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "\n",
        "    # generate the mask to distinguish real tokens from padding\n",
        "    # hint: inputs is positive for real tokens and 0 where they are padding\n",
        "    mask = (inputs != 0)\n",
        "\n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "\n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
        "    # note: attention heads is set to 1.\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "\n",
        "\n",
        "    return queries, keys, values, mask"
      ],
      "metadata": {
        "id": "iy1Htfdzqsf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating the Model"
      ],
      "metadata": {
        "id": "-BqD6LTgsL1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\n",
        "\n",
        "    Args:\n",
        "    input_vocab_size: int: vocab size of the input\n",
        "    target_vocab_size: int: vocab size of the target\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "    n_attention_heads: int: number of attention heads\n",
        "    attention_dropout: float, dropout for the attention layer\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "    An LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 0: call the input encoder function to create layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "    # Step 0: call the pre attention function to create layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "    # Step 1: create a serial network\n",
        "    model = tl.Serial(\n",
        "\n",
        "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "\n",
        "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "\n",
        "      # Step 4: prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "\n",
        "      # Step 5: run the AttentionQKV layer\n",
        "      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "\n",
        "      # Step 6: drop attention mask (i.e. index = None\n",
        "      tl.Select([0,2]),\n",
        "\n",
        "      # Step 7: run the rest of the RNN decoder\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
        "\n",
        "      # Step 8: prepare output by making it the right size\n",
        "      tl.Dense(target_vocab_size),\n",
        "\n",
        "      # Step 9: Log-softmax for output\n",
        "       tl.LogSoftmax()\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "NfkHPS3vsLSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the model to see the model architecture and it conforms standards\n",
        "model = NMTAttn()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioPYi9fJtCsD",
        "outputId": "abc51842-f925-42a6-d85e-b7a13134c127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial_in2_out2[\n",
            "  Select[0,1,0,1]_in2_out4\n",
            "  Parallel_in2_out2[\n",
            "    Serial[\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "    Serial[\n",
            "      Serial[\n",
            "        ShiftRight(1)\n",
            "      ]\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "  ]\n",
            "  PrepareAttentionInput_in3_out4\n",
            "  Serial_in4_out2[\n",
            "    Branch_in4_out3[\n",
            "      None\n",
            "      Serial_in4_out2[\n",
            "        _in4_out4\n",
            "        Serial_in4_out2[\n",
            "          Parallel_in3_out3[\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "          ]\n",
            "          PureAttention_in4_out2\n",
            "          Dense_1024\n",
            "        ]\n",
            "        _in2_out2\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Select[0,2]_in3_out2\n",
            "  LSTM_1024\n",
            "  LSTM_1024\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Training, Evaluation, Loop & run of the model"
      ],
      "metadata": {
        "id": "uL2GUcbgtUjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_task_function(train_batch_stream):\n",
        "    \"\"\"Returns a trax.training.TrainTask object.\n",
        "\n",
        "    Args:\n",
        "    train_batch_stream generator: labeled data generator\n",
        "\n",
        "    Returns:\n",
        "    A trax.training.TrainTask object.\n",
        "    \"\"\"\n",
        "    return training.TrainTask(\n",
        "\n",
        "        # use the train batch stream as labeled data\n",
        "        labeled_data= train_batch_stream,\n",
        "\n",
        "        # use the cross entropy loss\n",
        "        loss_layer= tl.CrossEntropyLoss(),\n",
        "\n",
        "        # use the Adam optimizer with learning rate of 0.01\n",
        "        optimizer= trax.optimizers.Adam(learning_rate = 0.01),\n",
        "\n",
        "        # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\n",
        "        # have 1000 warmup steps with a max value of 0.01\n",
        "        lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "\n",
        "        # have a checkpoint every 10 steps\n",
        "        n_steps_per_checkpoint= 10\n",
        "    )\n",
        "\n",
        "train_task = train_task_function(train_batch_stream)\n",
        "\n",
        "#Define the evaluation Task\n",
        "eval_task = training.EvalTask(\n",
        "\n",
        "    ## use the eval batch stream as labeled data\n",
        "    labeled_data=eval_batch_stream,\n",
        "\n",
        "    ## use the cross entropy loss and accuracy as metrics\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "# define the output directory\n",
        "output_dir = 'output_dir/'\n",
        "\n",
        "# remove old model if it exists. restarts training.\n",
        "!rm -f ~/output_dir/model.pkl.gz\n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)\n",
        "# NOTE: Execute the training loop.\n",
        "training_loop.run(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTAZnAVdtUKs",
        "outputId": "30aed0cb-5232-414d-9582-268a4d7e48b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:851: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 148492820\n",
            "Step      1: Ran 1 train steps in 136.92 secs\n",
            "Step      1: train CrossEntropyLoss |  10.42965794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step      1: eval  CrossEntropyLoss |  10.41868877\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 548.88 secs\n",
            "Step     10: train CrossEntropyLoss |  10.27014637\n",
            "Step     10: eval  CrossEntropyLoss |  9.96459675\n",
            "Step     10: eval          Accuracy |  0.03585050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing\n",
        "In order to save training time & other resources which are very limited in this use case, we use pre - trained model"
      ],
      "metadata": {
        "id": "dSs8AgSVwrBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "# initialize weights from a pre-trained model\n",
        "model.init_from_file(\"/content/drive/MyDrive/Project_Files/model.pkl.gz\", weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ],
      "metadata": {
        "id": "by_lh4jrwpZf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting the next index for the attention model"
      ],
      "metadata": {
        "id": "gVFIF8RHxGhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "\n",
        "    Args:\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"\n",
        "\n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    # calculate next power of 2 for padding length\n",
        "    padded_length = 2**np.log2(token_length + 1)\n",
        "\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + ([0] * int(padded_length))\n",
        "\n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (1, <padded_length>)\n",
        "    padded_with_batch = np.reshape(padded, (1, len(padded)))\n",
        "\n",
        "    # get the model prediction\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
        "\n",
        "    # get log probabilities slice for the next token\n",
        "    # (Hint: choose correct indices on the output)\n",
        "    log_probs = output[0,token_length,:]\n",
        "\n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs,temperature))\n",
        "\n",
        "    return symbol, float(log_probs[symbol])"
      ],
      "metadata": {
        "id": "Nt11axR-xkfx"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sampling Decoding\n",
        "the sampling_decode() function will call the next_symbol() function above several times until the next output is the end-of-sentence token (i.e. EOS). It takes in an input string and returns the translated version of that string."
      ],
      "metadata": {
        "id": "4g0XVh20z91i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"\n",
        "\n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "    # initialize an empty the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "\n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0\n",
        "\n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "\n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS:\n",
        "\n",
        "        # update the current output token by getting the index of the next word\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "\n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "\n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "    return cur_output_tokens, log_prob, sentence"
      ],
      "metadata": {
        "id": "FidR3TuNz9c3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the function\n",
        "sampling_decode(\"I love languages.\", NMTAttn=model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "metadata": {
        "id": "P7aBFOe11Tjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344d691a-ede2-439b-816c-1b4076b0b8ba"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([161, 12202, 5112, 3, 1], -0.0001735687255859375, 'Ich liebe Sprachen.')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Minimum Bayes-Risk Decoding\n",
        "Getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "1.   take several random samples\n",
        "2.   score each sample against all other samples\n",
        "3.   select the one with the highest score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3HMPHYXo1dWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "    \"\"\"Generates samples using sampling_decode()\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, list)\n",
        "            list of lists: token list per sample\n",
        "            list of floats: log probability per sample\n",
        "    \"\"\"\n",
        "    # define lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # run a for loop to generate n samples\n",
        "    for _ in range(n_samples):\n",
        "\n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol)\n",
        "\n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "\n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)\n",
        "\n",
        "    return samples, log_probs"
      ],
      "metadata": {
        "id": "VZU8hupr3IxJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the generate_samples function: generate 4 samples with the default temperature (0.6)\n",
        "generate_samples('how are you today?', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "metadata": {
        "id": "eBQ-YQ7N3iD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462cc509-dc5b-4629-d450-6455b0997e74"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[93, 30166, 705, 352, 102, 1],\n",
              "  [595, 30166, 4714, 352, 102, 1],\n",
              "  [595, 30166, 705, 352, 102, 1],\n",
              "  [595, 75, 67, 352, 102, 1]],\n",
              " [-1.1444091796875e-05,\n",
              "  -1.33514404296875e-05,\n",
              "  -9.5367431640625e-06,\n",
              "  -3.814697265625e-06])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing the Overlaps"
      ],
      "metadata": {
        "id": "BMWZ3UjR3rRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    \"\"\"Returns the Jaccard similarity between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list of int): tokenized version of the candidate translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"\n",
        "\n",
        "    # convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)\n",
        "\n",
        "    # get the set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "\n",
        "    # get the set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "\n",
        "    # divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "id": "BDU0t0mc34uB"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try using the function. remember the result here and compare with the next function below.\n",
        "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
      ],
      "metadata": {
        "id": "g1W7h9ok4FW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38705eee-fd55-41fe-e5f2-37ec2ab2bc44"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Average_Overlap\n",
        "This find the scores for the overalap generated samples"
      ],
      "metadata": {
        "id": "XEGJkobL5C9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        similarity_fn (function): similarity function used to compute the overlap\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        *ignore_params: additional parameters will be ignored\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "\n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "\n",
        "        # initialize overlap\n",
        "        overlap = 0\n",
        "\n",
        "        # run a for loop for each sample\n",
        "        for index_sample, sample in enumerate(samples):\n",
        "\n",
        "            # skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            # get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate,sample)\n",
        "\n",
        "            # add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "\n",
        "        # get the score for the candidate by computing the average\n",
        "        score = overlap/(len(samples)-1)\n",
        "\n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "mkpJDQUR4fqq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ],
      "metadata": {
        "id": "OsGdA5lc5Z-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed699875-2baf-46bc-dcdb-671eeafc60d8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.45, 1: 0.625, 2: 0.575}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Putting all together for the Minimum Bayes Risk\n"
      ],
      "metadata": {
        "id": "EBYwRqkP5ojH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, generate_samples=generate_samples, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
        "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        score_fn (function): function that generates the score for each sample\n",
        "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    # generate samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
        "\n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    # pass in the relevant parameters as shown in the function definition of\n",
        "    # the mean methods you developed earlier\n",
        "    scores = score_fn(similarity_fn,samples,log_probs)\n",
        "\n",
        "    # find the key with the highest score\n",
        "    max_score_key = max(scores, key = lambda x: scores[x])\n",
        "\n",
        "    # detokenize the token list associated with the max_score_key\n",
        "    translated_sentence = detokenize(samples[max_score_key], vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "    return (translated_sentence, max_score_key, scores)"
      ],
      "metadata": {
        "id": "T_1bxF-F5oMv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 1.0\n",
        "\n",
        "# a custom sentence\n",
        "your_sentence = 'She speaks English and German.'"
      ],
      "metadata": {
        "id": "j1H-1Wp36WuD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mbr_decode(your_sentence, 4, average_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "metadata": {
        "id": "jQklkWWq6fq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e9e19668-d816-4134-c59a-8a5e4374b2f4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sie spricht Englisch und Deutsch.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}